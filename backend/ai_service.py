import requests
import json
import time
import ssl
import certifi
from typing import Dict, Any, List, Optional
import logging
import urllib3

# ç¦ç”¨ SSL è­¦å‘Šï¼ˆä»£ç†ç¯å¢ƒä¸‹å¯èƒ½éœ€è¦ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é”™è¯¯ç æ˜ å°„ä¸ºå‹å¥½æç¤º
ERROR_MESSAGES = {
    429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼ŒAPI é…é¢å·²ç”¨å°½ã€‚è¯·ç¨åå†è¯•æˆ–æ›´æ¢æ¨¡å‹ï¼ˆæ¨èä½¿ç”¨ gemini-1.5-flashï¼‰",
    401: "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥é…ç½®",
    403: "API Key æƒé™ä¸è¶³æˆ–è¢«ç¦ç”¨",
    404: "æ¨¡å‹ä¸å­˜åœ¨æˆ– API è·¯å¾„é”™è¯¯",
    500: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•",
    503: "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
}

# å°è¯•å¯¼å…¥ httpxï¼ˆæ›´å¥½çš„ä»£ç†æ”¯æŒï¼‰
try:
    import httpx

    HAS_HTTPX = True
    logger.info("ä½¿ç”¨ httpx åº“è¿›è¡Œ HTTP è¯·æ±‚")
except ImportError:
    HAS_HTTPX = False
    logger.info("httpx æœªå®‰è£…ï¼Œä½¿ç”¨ requests åº“")


class AIService:

    @staticmethod
    def _get_proxies(proxy: str = None) -> dict:
        """
        æ„å»ºä»£ç†é…ç½®ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        """
        if not proxy:
            return None

        # ç¡®ä¿ä»£ç†åœ°å€æ ¼å¼æ­£ç¡®
        proxy = proxy.strip()
        if not proxy.startswith("http://") and not proxy.startswith("https://"):
            proxy = f"http://{proxy}"

        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy}")
        return {"http://": proxy, "https://": proxy}

    @staticmethod
    def _get_requests_proxies(proxy: str = None) -> dict:
        """
        æ„å»º requests åº“çš„ä»£ç†é…ç½®
        """
        if not proxy:
            return None

        proxy = proxy.strip()
        if not proxy.startswith("http://") and not proxy.startswith("https://"):
            proxy = f"http://{proxy}"

        return {"http": proxy, "https": proxy}

    @staticmethod
    def _make_request(method: str, url: str, proxy: str = None, **kwargs) -> dict:
        """
        ç»Ÿä¸€çš„ HTTP è¯·æ±‚æ–¹æ³•ï¼Œä¼˜å…ˆä½¿ç”¨ httpx
        """
        timeout = kwargs.pop("timeout", 60)

        if HAS_HTTPX:
            # ä½¿ç”¨ httpxï¼ˆæ›´å¥½çš„ä»£ç†å’Œ SSL æ”¯æŒï¼‰
            # æ„å»ºä»£ç†åœ°å€
            proxy_url = None
            if proxy:
                proxy = proxy.strip()
                if not proxy.startswith("http://") and not proxy.startswith("https://"):
                    proxy_url = f"http://{proxy}"
                else:
                    proxy_url = proxy

            try:
                with httpx.Client(
                    proxy=proxy_url,  # httpx æ–°ç‰ˆä½¿ç”¨ proxy è€Œä¸æ˜¯ proxies
                    timeout=timeout,
                    verify=False,  # è·³è¿‡ SSL éªŒè¯
                    follow_redirects=True,
                ) as client:
                    if method.upper() == "GET":
                        response = client.get(url, **kwargs)
                    else:
                        response = client.post(url, **kwargs)
                    response.raise_for_status()
                    return response.json()
            except httpx.HTTPStatusError as e:
                # è½¬æ¢ä¸ºç±»ä¼¼ requests çš„å¼‚å¸¸æ ¼å¼
                raise requests.exceptions.HTTPError(response=e.response)
            except httpx.ProxyError as e:
                raise requests.exceptions.ProxyError(str(e))
            except httpx.ConnectError as e:
                raise requests.exceptions.ConnectionError(str(e))
            except httpx.TimeoutException as e:
                raise requests.exceptions.Timeout(str(e))
        else:
            # å›é€€åˆ° requests
            proxies = AIService._get_requests_proxies(proxy)
            if method.upper() == "GET":
                response = requests.get(
                    url, proxies=proxies, timeout=timeout, verify=False, **kwargs
                )
            else:
                response = requests.post(
                    url, proxies=proxies, timeout=timeout, verify=False, **kwargs
                )
            response.raise_for_status()
            return response.json()

    @staticmethod
    def get_models(provider: str, api_key: str, proxy: str = None) -> List[Dict]:
        """
        è·å–æŒ‡å®šæä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        try:
            if provider.lower() == "openai" or provider.lower() == "gpt":
                return AIService._get_openai_models(api_key, proxy)
            elif provider.lower() == "gemini":
                return AIService._get_gemini_models(api_key, proxy)
            elif provider.lower() == "claude":
                return AIService._get_claude_models(api_key, proxy)
            else:
                return []
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    @staticmethod
    def _get_openai_models(api_key: str, proxy: str = None) -> List[Dict]:
        """è·å– OpenAI æ¨¡å‹åˆ—è¡¨"""
        url = "https://api.openai.com/v1/models"
        headers = {"Authorization": f"Bearer {api_key}"}
        data = AIService._make_request(
            "GET", url, proxy=proxy, headers=headers, timeout=30
        )
        # è¿‡æ»¤å‡º GPT ç›¸å…³æ¨¡å‹
        models = []
        for m in data.get("data", []):
            model_id = m.get("id", "")
            if "gpt" in model_id.lower():
                models.append({"id": model_id, "name": model_id})
        # æŒ‰åç§°æ’åº
        models.sort(key=lambda x: x["id"])
        return models

    @staticmethod
    def _get_gemini_models(api_key: str, proxy: str = None) -> List[Dict]:
        """è·å– Gemini æ¨¡å‹åˆ—è¡¨"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
        logger.info(f"è¯·æ±‚ Gemini æ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨ä»£ç†: {proxy}")
        data = AIService._make_request("GET", url, proxy=proxy, timeout=30)
        models = []
        for m in data.get("models", []):
            name = m.get("name", "").replace("models/", "")
            display_name = m.get("displayName", name)
            # åªä¿ç•™æ”¯æŒ generateContent çš„æ¨¡å‹
            methods = m.get("supportedGenerationMethods", [])
            if "generateContent" in methods:
                models.append({"id": name, "name": display_name})
        return models

    @staticmethod
    def _get_claude_models(api_key: str, proxy: str = None) -> List[Dict]:
        """Claude æ²¡æœ‰å…¬å¼€çš„æ¨¡å‹åˆ—è¡¨ APIï¼Œè¿”å›å¸¸ç”¨æ¨¡å‹"""
        return [
            {"id": "claude-3-5-sonnet-20241022", "name": "Claude 3.5 Sonnet"},
            {"id": "claude-3-5-haiku-20241022", "name": "Claude 3.5 Haiku"},
            {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus"},
            {"id": "claude-3-sonnet-20240229", "name": "Claude 3 Sonnet"},
            {"id": "claude-3-haiku-20240307", "name": "Claude 3 Haiku"},
        ]

    @staticmethod
    def _format_volume(vol: int) -> str:
        """æ ¼å¼åŒ–æˆäº¤é‡ä¸ºæ˜“è¯»æ ¼å¼"""
        if vol >= 100000000:
            return f"{vol / 100000000:.2f}äº¿"
        elif vol >= 10000:
            return f"{vol / 10000:.2f}ä¸‡"
        return str(vol)

    @staticmethod
    def format_data_for_prompt(
        basic: Dict,
        minute_data: List[Dict],
        kline_data: List[Dict],
        extra_info: Optional[Dict] = None,
        market_data: Optional[Dict] = None,
        trade_history: Optional[List[Dict]] = None,
        ai_history: Optional[List[Dict]] = None,
    ) -> str:
        """
        å°†è‚¡ç¥¨æ•°æ®æ ¼å¼åŒ–ä¸º LLM æç¤ºè¯ï¼Œé‡ç‚¹çªå‡ºæˆäº¤é‡ä¿¡æ¯
        åŒ…å«ç”¨æˆ·äº¤æ˜“è®°å½•å’Œå†å² AI åˆ†æè®°å½•ï¼ˆç”¨äºç²¾å‡†åˆ†æï¼‰
        """
        prompt_parts = []

        # 1. åŸºæœ¬ä¿¡æ¯
        prompt_parts.append(f"### è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯\n")
        prompt_parts.append(f"- ä»£ç : {basic.get('code')}")
        prompt_parts.append(f"- åç§°: {basic.get('name')}")
        prompt_parts.append(f"- å½“å‰ä»·: {basic.get('price')}")
        prompt_parts.append(f"- æ¶¨è·Œå¹…: {basic.get('change_percent')}%")
        prompt_parts.append(f"- ä»Šæ—¥æœ€é«˜: {basic.get('high')}")
        prompt_parts.append(f"- ä»Šæ—¥æœ€ä½: {basic.get('low')}")
        prompt_parts.append(f"- æ˜¨æ”¶: {basic.get('pre_close')}")
        if basic.get("volume"):
            prompt_parts.append(
                f"- æˆäº¤é‡: {AIService._format_volume(int(float(basic.get('volume', 0))))}"
            )
        if basic.get("amount"):
            prompt_parts.append(
                f"- æˆäº¤é¢: {AIService._format_volume(int(float(basic.get('amount', 0))))}"
            )

        # 2. ç”¨æˆ·é™„åŠ ä¿¡æ¯ï¼ˆç²¾å‡†åˆ†ææ¨¡å¼ï¼‰
        if extra_info:
            prompt_parts.append(f"\n### ç”¨æˆ·é™„åŠ ä¿¡æ¯\n")
            if extra_info.get("cost_price"):
                prompt_parts.append(f"- æŒä»“æˆæœ¬: {extra_info['cost_price']}")
            if extra_info.get("position"):
                prompt_parts.append(f"- æŒä»“æ•°é‡: {extra_info['position']}")
            if extra_info.get("take_profit"):
                prompt_parts.append(f"- æ­¢ç›ˆä½ç½®: {extra_info['take_profit']}")
            if extra_info.get("stop_loss"):
                prompt_parts.append(f"- æ­¢æŸä½ç½®: {extra_info['stop_loss']}")
            if extra_info.get("extra_text"):
                prompt_parts.append(
                    f"- è¡¥å……è¯´æ˜/æ–°é—»/æ”¿ç­–: \n{extra_info['extra_text']}"
                )

        # 3. æ—¥Kçº¿æ•°æ®ï¼ˆé‡ç‚¹å±•ç¤ºæˆäº¤é‡ï¼‰
        prompt_parts.append(f"\n### è¿‘æœŸæ—¥Kçº¿æ•°æ®ï¼ˆä»·é‡é…åˆåˆ†æï¼‰\n")
        if kline_data:
            last_k = kline_data[-15:]  # å±•ç¤ºæœ€è¿‘15å¤©
            prompt_parts.append(
                f"æœ€è¿‘ {len(last_k)} ä¸ªäº¤æ˜“æ—¥æ•°æ® (å…±æä¾› {len(kline_data)} å¤©å†å²):"
            )

            # è®¡ç®—å¹³å‡æˆäº¤é‡ç”¨äºå¯¹æ¯”
            volumes = [k["volume"] for k in kline_data[-30:] if k.get("volume")]
            avg_volume = sum(volumes) / len(volumes) if volumes else 0

            header = "| æ—¥æœŸ | å¼€ç›˜ | æ”¶ç›˜ | æœ€é«˜ | æœ€ä½ | æ¶¨è·Œå¹… | æˆäº¤é‡ | é‡æ¯” |"
            prompt_parts.append(header)
            prompt_parts.append("|---|---|---|---|---|---|---|---|")

            prev_close = None
            if len(kline_data) > 15:
                prev_close = kline_data[-16]["close"]

            for k in last_k:
                change = ""
                if prev_close:
                    pct = ((k["close"] - prev_close) / prev_close) * 100
                    change = f"{pct:+.2f}%"
                prev_close = k["close"]

                vol = k.get("volume", 0)
                vol_ratio = f"{vol / avg_volume:.2f}" if avg_volume > 0 else "-"
                vol_str = AIService._format_volume(vol)

                prompt_parts.append(
                    f"| {k['date']} | {k['open']} | {k['close']} | {k['high']} | {k['low']} | {change} | {vol_str} | {vol_ratio} |"
                )

            # æˆäº¤é‡è¶‹åŠ¿åˆ†æ
            recent_5_vol = (
                sum([k["volume"] for k in last_k[-5:]]) / 5 if len(last_k) >= 5 else 0
            )
            recent_10_vol = (
                sum([k["volume"] for k in last_k[-10:]]) / 10
                if len(last_k) >= 10
                else 0
            )

            prompt_parts.append(f"\n**æˆäº¤é‡åˆ†æ:**")
            prompt_parts.append(
                f"- 30æ—¥å¹³å‡æˆäº¤é‡: {AIService._format_volume(int(avg_volume))}"
            )
            prompt_parts.append(
                f"- è¿‘5æ—¥å¹³å‡æˆäº¤é‡: {AIService._format_volume(int(recent_5_vol))}"
            )
            prompt_parts.append(
                f"- è¿‘10æ—¥å¹³å‡æˆäº¤é‡: {AIService._format_volume(int(recent_10_vol))}"
            )

            if recent_5_vol > avg_volume * 1.5:
                prompt_parts.append(f"- âš ï¸ è¿‘æœŸæˆäº¤é‡æ˜æ˜¾æ”¾å¤§ï¼Œéœ€å…³æ³¨")
            elif recent_5_vol < avg_volume * 0.5:
                prompt_parts.append(f"- âš ï¸ è¿‘æœŸæˆäº¤é‡æ˜æ˜¾èç¼©")

        # 4. åˆ†æ—¶æ•°æ®ï¼ˆåŒ…å«æˆäº¤é‡ï¼‰
        prompt_parts.append(f"\n### è¿‘æœŸåˆ†æ—¶èµ°åŠ¿ï¼ˆå«æˆäº¤é‡ï¼‰\n")
        if minute_data:
            # æŒ‰æ—¥æœŸåˆ†ç»„
            dates = {}
            for m in minute_data:
                d = m["date"]
                if d not in dates:
                    dates[d] = []
                dates[d].append(m)

            # åªå±•ç¤ºæœ€è¿‘2å¤©çš„åˆ†æ—¶æ•°æ®
            sorted_dates = sorted(dates.keys(), reverse=True)[:2]

            for d in sorted(sorted_dates):
                items = dates[d]
                prompt_parts.append(f"#### æ—¥æœŸ: {d}")

                # è®¡ç®—å½“æ—¥æˆäº¤é‡ç»Ÿè®¡
                total_vol = sum([item.get("volume", 0) for item in items])
                prompt_parts.append(
                    f"å½“æ—¥æ€»æˆäº¤é‡: {AIService._format_volume(total_vol)}"
                )

                header = "| æ—¶é—´ | ä»·æ ¼ | å‡ä»· | æˆäº¤é‡ | ç´¯è®¡æˆäº¤é‡ |"
                prompt_parts.append(header)
                prompt_parts.append("|---|---|---|---|---|")

                cumulative_vol = 0
                for i, item in enumerate(items):
                    vol = item.get("volume", 0)
                    cumulative_vol += vol
                    # æ¯15åˆ†é’Ÿé‡‡æ ·ä¸€æ¬¡ï¼Œä»¥åŠå¼€ç›˜å’Œæ”¶ç›˜
                    if i == 0 or i == len(items) - 1 or i % 15 == 0:
                        prompt_parts.append(
                            f"| {item['time']} | {item['price']} | {item.get('avg_price', '-')} | {AIService._format_volume(vol)} | {AIService._format_volume(cumulative_vol)} |"
                        )

        # 5. å¤§ç›˜æ•°æ®
        if market_data:
            prompt_parts.append(f"\n### å¤§ç›˜ç¯å¢ƒ\n")

            # æŒ‡æ•°æ•°æ®
            index_data = market_data.get("index", {})
            if index_data:
                prompt_parts.append("**ä¸»è¦æŒ‡æ•°:**")
                for code, idx in index_data.items():
                    if idx:
                        prompt_parts.append(
                            f"- {idx.get('name', code)}: {idx.get('price')} ({idx.get('change_percent')}%)"
                        )

        # 6. ç”¨æˆ·äº¤æ˜“è®°å½•ï¼ˆç²¾å‡†åˆ†ææ—¶æä¾›ï¼‰
        if trade_history:
            prompt_parts.append(f"\n### ç”¨æˆ·äº¤æ˜“è®°å½•\n")
            prompt_parts.append("ä»¥ä¸‹æ˜¯ç”¨æˆ·åœ¨è¯¥è‚¡ç¥¨ä¸Šçš„å†å²æ“ä½œè®°å½•ï¼š")
            prompt_parts.append("| æ—¶é—´ | ç±»å‹ | ä»·æ ¼ | æ‰‹æ•° | æ“ä½œåŸå›  |")
            prompt_parts.append("|---|---|---|---|---|")
            for t in trade_history:
                type_map = {"B": "ä¹°å…¥", "S": "å–å‡º", "T": "åšT"}
                type_str = type_map.get(t["type"], t["type"])
                prompt_parts.append(
                    f"| {t['trade_time']} | {type_str} | {t['price']} | {t['quantity']} | {t['reason']} |"
                )
            prompt_parts.append("\nè¯·ç»“åˆç”¨æˆ·çš„æŒä»“æˆæœ¬å’Œæ“ä½œå†å²ç»™å‡ºå»ºè®®ã€‚")

        # 7. å†å² AI åˆ†æè®°å½•ï¼ˆç²¾å‡†åˆ†ææ—¶æä¾›ï¼Œç”¨äºæ¨¡å‹è‡ªæˆ‘å¤ç›˜ï¼‰
        if ai_history:
            prompt_parts.append(f"\n### å†å² AI åˆ†æè®°å½•\n")
            prompt_parts.append("ä»¥ä¸‹æ˜¯ä¹‹å‰å¯¹è¯¥è‚¡ç¥¨çš„åˆ†æè®°å½•ï¼Œè¯·å‚è€ƒå¹¶å¤ç›˜ï¼š")
            prompt_parts.append("| æ—¶é—´ | ä¿¡å· | æ‘˜è¦ |")
            prompt_parts.append("|---|---|---|")
            signal_map = {"bullish": "çœ‹æ¶¨ğŸ“ˆ", "cautious": "è°¨æ…âš ï¸", "bearish": "çœ‹è·ŒğŸ“‰"}
            for a in ai_history:
                signal_str = signal_map.get(a["signal"], a["signal"])
                prompt_parts.append(f"| {a['datetime']} | {signal_str} | {a['summary']} |")
            prompt_parts.append("\nè¯·å¯¹æ¯”ä¹‹å‰çš„åˆ†æï¼Œè¯´æ˜èµ°åŠ¿æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œå¹¶ç»™å‡ºæ–°çš„åˆ¤æ–­ã€‚")

        prompt_parts.append(f"\n### åˆ†æè¦æ±‚\n")
        prompt_parts.append("è¯·é‡ç‚¹åˆ†æï¼š")
        prompt_parts.append("1. ä»·é‡é…åˆå…³ç³»ï¼ˆæ”¾é‡ä¸Šæ¶¨/ç¼©é‡ä¸‹è·Œç­‰ï¼‰")
        prompt_parts.append("2. æˆäº¤é‡å¼‚åŠ¨æƒ…å†µ")
        prompt_parts.append("3. ç»“åˆå¤§ç›˜ç¯å¢ƒåˆ¤æ–­ä¸ªè‚¡èµ°åŠ¿")
        prompt_parts.append("4. çŸ­æœŸè¶‹åŠ¿åˆ¤æ–­")
        prompt_parts.append("5. æ“ä½œå»ºè®®ï¼ˆä¹°å…¥/æŒæœ‰/å–å‡ºï¼‰")

        return "\n".join(prompt_parts)

    @staticmethod
    def call_llm(
        provider: str,
        api_key: str,
        model: str,
        prompt: str,
        proxy: str = None,
        max_retries: int = 3,
    ) -> str:
        """
        è°ƒç”¨ LLM APIï¼Œæ”¯æŒä»£ç†å’Œé‡è¯•æœºåˆ¶
        """
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œæ“…é•¿æŠ€æœ¯é¢åˆ†æå’ŒåŸºæœ¬é¢åˆ†æã€‚è¯·æ ¹æ®æä¾›çš„è‚¡ç¥¨æ•°æ®ï¼Œç»™å‡ºä¸“ä¸šçš„è¶‹åŠ¿é¢„æµ‹å’Œæ“ä½œå»ºè®®ã€‚é‡ç‚¹å…³æ³¨æˆäº¤é‡å˜åŒ–ä¸ä»·æ ¼èµ°åŠ¿çš„é…åˆå…³ç³»ã€‚è¾“å‡ºæ ¼å¼ä½¿ç”¨Markdownã€‚"

        last_error = None
        for attempt in range(max_retries):
            try:
                if provider.lower() == "openai" or provider.lower() == "gpt":
                    return AIService._call_openai(
                        api_key, model, system_prompt, prompt, proxy
                    )
                elif provider.lower() == "gemini":
                    return AIService._call_gemini(
                        api_key, model, system_prompt, prompt, proxy
                    )
                elif provider.lower() == "claude":
                    return AIService._call_claude(
                        api_key, model, system_prompt, prompt, proxy
                    )
                else:
                    return f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}"
            except requests.exceptions.HTTPError as e:
                last_error = e
                status_code = getattr(getattr(e, "response", None), "status_code", 0)

                # è·å–å‹å¥½é”™è¯¯æç¤º
                friendly_msg = ERROR_MESSAGES.get(
                    status_code, f"HTTP é”™è¯¯ {status_code}"
                )
                logger.error(
                    f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {friendly_msg}"
                )

                # 429 é”™è¯¯éœ€è¦ç­‰å¾…åé‡è¯•
                if status_code == 429 and attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # é€’å¢ç­‰å¾…æ—¶é—´: 5s, 10s, 15s
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue

                # å…¶ä»–é”™è¯¯ç›´æ¥è¿”å›å‹å¥½æç¤º
                return f"åˆ†æå¤±è´¥: {friendly_msg}"

            except requests.exceptions.ProxyError as e:
                last_error = e
                logger.error(f"ä»£ç†è¿æ¥å¤±è´¥: {e}")
                return (
                    f"åˆ†æå¤±è´¥: ä»£ç†è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†åœ°å€æ˜¯å¦æ­£ç¡®ï¼ˆå½“å‰: {proxy}ï¼‰"
                )

            except requests.exceptions.ConnectionError as e:
                last_error = e
                error_str = str(e)
                logger.error(f"è¿æ¥å¤±è´¥: {e}")

                if "ProxyError" in error_str or "proxy" in error_str.lower():
                    return f"åˆ†æå¤±è´¥: ä»£ç†è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†æ˜¯å¦æ­£å¸¸è¿è¡Œï¼ˆå½“å‰: {proxy}ï¼‰"
                elif "ConnectTimeout" in error_str:
                    return f"åˆ†æå¤±è´¥: è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†é…ç½®"
                else:
                    return f"åˆ†æå¤±è´¥: ç½‘ç»œè¿æ¥å¤±è´¥ - {error_str[:100]}"

            except requests.exceptions.Timeout:
                last_error = "è¯·æ±‚è¶…æ—¶"
                logger.error(f"LLMè°ƒç”¨è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return "åˆ†æå¤±è´¥: è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†é…ç½®"

            except Exception as e:
                last_error = e
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                return f"åˆ†æå¤±è´¥: {str(e)}"

        return f"åˆ†æå¤±è´¥: é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥ - {last_error}"

    @staticmethod
    def _call_openai(
        api_key: str,
        model: str,
        system_prompt: str,
        user_prompt: str,
        proxy: str = None,
    ) -> str:
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        }
        res_json = AIService._make_request(
            "POST", url, proxy=proxy, headers=headers, json=data, timeout=90
        )
        return res_json["choices"][0]["message"]["content"]

    @staticmethod
    def _call_gemini(
        api_key: str,
        model: str,
        system_prompt: str,
        user_prompt: str,
        proxy: str = None,
    ) -> str:
        if not model:
            model = "gemini-pro"

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}

        full_prompt = f"{system_prompt}\n\nUser Request:\n{user_prompt}"

        data = {"contents": [{"parts": [{"text": full_prompt}]}]}

        logger.info(f"è°ƒç”¨ Gemini APIï¼Œæ¨¡å‹: {model}ï¼Œä»£ç†: {proxy}")
        res_json = AIService._make_request(
            "POST", url, proxy=proxy, headers=headers, json=data, timeout=90
        )
        try:
            return res_json["candidates"][0]["content"]["parts"][0]["text"]
        except (KeyError, IndexError):
            return "Gemini è¿”å›äº†æ— æ³•è§£æçš„æ•°æ®: " + json.dumps(res_json)

    @staticmethod
    def _call_claude(
        api_key: str,
        model: str,
        system_prompt: str,
        user_prompt: str,
        proxy: str = None,
    ) -> str:
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json",
        }

        data = {
            "model": model,
            "system": system_prompt,
            "messages": [{"role": "user", "content": user_prompt}],
            "max_tokens": 4096,
        }

        res_json = AIService._make_request(
            "POST", url, proxy=proxy, headers=headers, json=data, timeout=90
        )
        return res_json["content"][0]["text"]

    @staticmethod
    def extract_signal_from_result(result: str) -> Dict[str, str]:
        """
        ä»åˆ†æç»“æœä¸­æå–ä¿¡å·å’Œæ‘˜è¦
        å°è¯•è§£æç»“æœä¸­çš„ JSON å—ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…³é”®è¯åŒ¹é…
        """
        import re
        
        # å°è¯•ä»ç»“æœä¸­æå– JSON å—
        json_pattern = r'```json\s*(\{[^`]+\})\s*```'
        match = re.search(json_pattern, result, re.DOTALL)
        if match:
            try:
                data = json.loads(match.group(1))
                return {
                    "signal": data.get("signal", "cautious"),
                    "summary": data.get("summary", "")[:100]
                }
            except:
                pass
        
        # å°è¯•ç›´æ¥è§£æ JSONï¼ˆå¯èƒ½æ²¡æœ‰ä»£ç å—åŒ…è£¹ï¼‰
        json_inline = r'\{\s*"signal"\s*:\s*"(\w+)"\s*,\s*"summary"\s*:\s*"([^"]+)"\s*\}'
        match = re.search(json_inline, result)
        if match:
            return {
                "signal": match.group(1),
                "summary": match.group(2)[:100]
            }
        
        # å…³é”®è¯åŒ¹é…ä½œä¸ºåå¤‡æ–¹æ¡ˆ
        result_lower = result.lower()
        signal = "cautious"  # é»˜è®¤è°¨æ…
        
        # çœ‹æ¶¨å…³é”®è¯
        bullish_keywords = ["çœ‹æ¶¨", "ä¹°å…¥", "å¼ºåŠ¿", "çªç ´", "ä¸Šæ¶¨", "bullish", "buy", "å»ºè®®ä¹°å…¥", "é€¢ä½ä¹°å…¥"]
        # çœ‹è·Œå…³é”®è¯
        bearish_keywords = ["çœ‹è·Œ", "å–å‡º", "å¼±åŠ¿", "ä¸‹è·Œ", "bearish", "sell", "å»ºè®®å–å‡º", "å‡ä»“"]
        
        bullish_count = sum(1 for kw in bullish_keywords if kw in result_lower)
        bearish_count = sum(1 for kw in bearish_keywords if kw in result_lower)
        
        if bullish_count > bearish_count + 1:
            signal = "bullish"
        elif bearish_count > bullish_count + 1:
            signal = "bearish"
        
        # æå–æ‘˜è¦ï¼ˆå–ç¬¬ä¸€æ®µæœ‰æ„ä¹‰çš„æ–‡å­—ï¼‰
        lines = [l.strip() for l in result.split('\n') if l.strip() and not l.startswith('#')]
        summary = lines[0][:100] if lines else "åˆ†æå®Œæˆ"
        
        return {"signal": signal, "summary": summary}

    @staticmethod
    def call_llm_with_signal(
        provider: str,
        api_key: str,
        model: str,
        prompt: str,
        proxy: str = None,
        max_retries: int = 3,
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM å¹¶è¿”å›ç»“æ„åŒ–ç»“æœï¼ˆåŒ…å«ä¿¡å·ï¼‰
        è¿”å›: {"result": str, "signal": str, "summary": str}
        """
        # åœ¨ prompt æœ«å°¾æ·»åŠ ç»“æ„åŒ–è¾“å‡ºè¦æ±‚
        structured_prompt = prompt + """

### è¾“å‡ºæ ¼å¼è¦æ±‚

è¯·åœ¨åˆ†æç»“æŸåï¼Œé¢å¤–è¾“å‡ºä¸€ä¸ª JSON å—ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "signal": "bullish/cautious/bearish",
  "summary": "ä¸€å¥è¯æ€»ç»“ï¼ˆ50å­—ä»¥å†…ï¼‰"
}
```

signal å–å€¼è¯´æ˜ï¼š
- bullish: çœ‹æ¶¨ï¼Œå»ºè®®ä¹°å…¥æˆ–æŒæœ‰
- cautious: è°¨æ…ï¼Œå»ºè®®è§‚æœ›
- bearish: çœ‹è·Œï¼Œå»ºè®®å–å‡ºæˆ–å‡ä»“
"""
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œæ“…é•¿æŠ€æœ¯é¢åˆ†æå’ŒåŸºæœ¬é¢åˆ†æã€‚è¯·æ ¹æ®æä¾›çš„è‚¡ç¥¨æ•°æ®ï¼Œç»™å‡ºä¸“ä¸šçš„è¶‹åŠ¿é¢„æµ‹å’Œæ“ä½œå»ºè®®ã€‚é‡ç‚¹å…³æ³¨æˆäº¤é‡å˜åŒ–ä¸ä»·æ ¼èµ°åŠ¿çš„é…åˆå…³ç³»ã€‚è¾“å‡ºæ ¼å¼ä½¿ç”¨Markdownã€‚"

        last_error = None
        for attempt in range(max_retries):
            try:
                if provider.lower() == "openai" or provider.lower() == "gpt":
                    result = AIService._call_openai(
                        api_key, model, system_prompt, structured_prompt, proxy
                    )
                elif provider.lower() == "gemini":
                    result = AIService._call_gemini(
                        api_key, model, system_prompt, structured_prompt, proxy
                    )
                elif provider.lower() == "claude":
                    result = AIService._call_claude(
                        api_key, model, system_prompt, structured_prompt, proxy
                    )
                else:
                    return {"result": f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}", "signal": "cautious", "summary": ""}
                
                # æå–ä¿¡å·
                signal_data = AIService.extract_signal_from_result(result)
                return {
                    "result": result,
                    "signal": signal_data["signal"],
                    "summary": signal_data["summary"]
                }
                
            except requests.exceptions.HTTPError as e:
                last_error = e
                status_code = getattr(getattr(e, "response", None), "status_code", 0)
                friendly_msg = ERROR_MESSAGES.get(status_code, f"HTTP é”™è¯¯ {status_code}")
                logger.error(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {friendly_msg}")

                if status_code == 429 and attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                return {"result": f"åˆ†æå¤±è´¥: {friendly_msg}", "signal": "cautious", "summary": ""}

            except requests.exceptions.ProxyError as e:
                return {"result": f"åˆ†æå¤±è´¥: ä»£ç†è¿æ¥å¤±è´¥ï¼ˆå½“å‰: {proxy}ï¼‰", "signal": "cautious", "summary": ""}

            except requests.exceptions.ConnectionError as e:
                error_str = str(e)
                if "ProxyError" in error_str or "proxy" in error_str.lower():
                    return {"result": f"åˆ†æå¤±è´¥: ä»£ç†è¿æ¥å¤±è´¥", "signal": "cautious", "summary": ""}
                return {"result": f"åˆ†æå¤±è´¥: ç½‘ç»œè¿æ¥å¤±è´¥", "signal": "cautious", "summary": ""}

            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return {"result": "åˆ†æå¤±è´¥: è¯·æ±‚è¶…æ—¶", "signal": "cautious", "summary": ""}

            except Exception as e:
                return {"result": f"åˆ†æå¤±è´¥: {str(e)}", "signal": "cautious", "summary": ""}

        return {"result": f"åˆ†æå¤±è´¥: é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥", "signal": "cautious", "summary": ""}
